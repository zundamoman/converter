import streamlit as st
import geopandas as gpd
import pandas as pd
import json
import tempfile
import zipfile
import os
import io
import re
import configparser
import shutil
import shapefile  # pip install pyshp
import struct
import numpy as np
from collections import defaultdict
from shapely.geometry import shape, Polygon, MultiPolygon, LineString

# --- ãƒšãƒ¼ã‚¸åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="Agri Data Converter", layout="wide")

# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ¡ãƒ¼ã‚«ãƒ¼é¸æŠ
# ==========================================
st.sidebar.title("ğŸšœ Agri Data Converter")
maker = st.sidebar.radio("ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„", ["DJI", "ãƒˆãƒ—ã‚³ãƒ³"])

st.title(f"{maker} ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ„ãƒ¼ãƒ«")
st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã€ã€Œå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨å¤‰æ›ãƒ»ä¿®å¾©ãŒå§‹ã¾ã‚Šã¾ã™ã€‚")

# ==========================================
# DJI ã®ã‚¿ãƒ–æ§‹æˆ
# ==========================================
if maker == "DJI":
    tab1, = st.tabs(["ğŸš DJI å¢ƒç•Œç·šå¤‰æ›"])

    with tab1:
        st.subheader("DJI å¢ƒç•Œç·šãƒ‡ãƒ¼ã‚¿ â†’ SHP å¤‰æ›")
        st.write("DJIã®ã€Œåœƒå ´ãƒ‡ãƒ¼ã‚¿ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        uploaded_files_dji = st.file_uploader("DJIãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ­ãƒƒãƒ—", accept_multiple_files=True, key="dji")

        if uploaded_files_dji:
            if st.button("ğŸš€ DJIãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å¤‰æ›ã™ã‚‹", key="btn_dji"):
                zip_buffer = io.BytesIO()
                success_count = 0
                
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    with tempfile.TemporaryDirectory() as tmpdir:
                        for uploaded_file in uploaded_files_dji:
                            try:
                                text_content = uploaded_file.read().decode("utf-8")
                                json_match = re.search(r'\{.*\}', text_content, re.DOTALL)
                                if not json_match: continue
                                
                                data = json.loads(json_match.group(0))
                                features = []
                                for feat in data.get("features", []):
                                    if "Polygon" not in feat.get("geometry", {}).get("type", ""): continue
                                    geom = shape(feat["geometry"])
                                    if geom.has_z:
                                        geom = Polygon([(p[0], p[1]) for p in geom.exterior.coords])
                                    
                                    props = {str(k): str(v) for k, v in feat.get("properties", {}).items()}
                                    props['geometry'] = geom
                                    features.append(props)

                                if features:
                                    base_name = os.path.splitext(uploaded_file.name)[0]
                                    gdf = gpd.GeoDataFrame(features, crs="EPSG:4326")
                                    
                                    shp_path = os.path.join(tmpdir, base_name + ".shp")
                                    gdf.to_file(shp_path, driver='ESRI Shapefile', encoding='utf-8')
                                    for ext in ['.shp', '.shx', '.dbf', '.prj']:
                                        f_path = os.path.join(tmpdir, base_name + ext)
                                        if os.path.exists(f_path):
                                            zf.write(f_path, arcname=f"{base_name}/{base_name}{ext}")
                                    success_count += 1
                            except Exception: 
                                continue

                if success_count > 0:
                    st.success(f"âœ… {success_count} ä»¶å¤‰æ›å®Œäº†")
                    st.download_button("ğŸ“¥ DJI SHPä¿å­˜ (.zip)", zip_buffer.getvalue(), "dji_converted.zip", key="dl_dji")
                else:
                    st.error("å¤‰æ›å¯èƒ½ãªãƒãƒªã‚´ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ==========================================
# ãƒˆãƒ—ã‚³ãƒ³ ã®ã‚¿ãƒ–æ§‹æˆ
# ==========================================
elif maker == "ãƒˆãƒ—ã‚³ãƒ³":
    tab0, tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ›°ï¸ CRVåº§æ¨™è§£æ",
        "ğŸ“ˆ ãƒˆãƒ—ã‚³ãƒ³(æ›²ç·šå¯¾å¿œ)ä¸€æ‹¬å¤‰æ›",
        "ğŸšœ ãƒˆãƒ—ã‚³ãƒ³ A-Bãƒ©ã‚¤ãƒ³å¤‰æ›",
        "ğŸ”§ SHPä¸€æ‹¬ä¿®å¾©",
        "ğŸ“‚ ãƒˆãƒ—ã‚³ãƒ³ã¾ã¨ã‚ã¦å¤‰æ›"
    ])

    # --- ã‚¿ãƒ–0ï¼šãƒˆãƒ—ã‚³ãƒ³CRV çµ¶å¯¾åº§æ¨™ãƒ»è‡ªå‹•è§£æãƒ„ãƒ¼ãƒ« ---
    with tab0:
        st.subheader("ğŸ›°ï¸ ãƒˆãƒ—ã‚³ãƒ³CRV çµ¶å¯¾åº§æ¨™ãƒ»è‡ªå‹•è§£æ")
        st.write("FJDynamicsã¸ã®å®Œå…¨è‡ªå‹•å¤‰æ›ã‚’ç›®æŒ‡ã—ã€ãƒ˜ãƒƒãƒ€å†…ã®éš ã—åº§æ¨™ã‚’ç‰¹å®šã—ã¾ã™ã€‚")
        u_crv_debug = st.file_uploader(".crvãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è§£æç”¨)", type=['crv'], key="crv_debug")

        if u_crv_debug:
            binary_data = u_crv_debug.read()
            header = binary_data[:64]
            
            st.subheader("1. éš ã‚ŒãŸåº§æ¨™ã®æ¤œç´¢çµæœ (Double 64bit)")
            found_coords = []
            for i in range(len(header) - 8):
                val = struct.unpack('<d', header[i:i+8])[0]
                if (20.0 < val < 50.0) or (120.0 < val < 150.0):
                    found_coords.append({"Offset (Hex)": hex(i), "Found Value": val, "Type": "Coordinate?"})

            if found_coords:
                st.success("âœ… åº§æ¨™ã‚‰ã—ãæ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼")
                st.table(pd.DataFrame(found_coords))
            else:
                st.warning("ãƒ˜ãƒƒãƒ€å†…ã«ç›´æ¥çš„ãªç·¯åº¦çµŒåº¦(Double)ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            st.subheader("2. æ•´æ•°å€¤(Int32)ã«ã‚ˆã‚‹åº§æ¨™ä¿æŒã®å¯èƒ½æ€§")
            ints = []
            for i in range(0, 32, 4):
                val = struct.unpack('<i', header[i:i+4])[0]
                ints.append({"Offset": hex(i), "Value": val})
            st.table(pd.DataFrame(ints))

    # --- ã‚¿ãƒ–1ï¼šãƒˆãƒ—ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å¤‰æ› (FJDå¯¾å¿œãƒ­ã‚¸ãƒƒã‚¯æ­è¼‰) ---
    with tab1:
        st.subheader("ãƒˆãƒ—ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å¤‰æ› (FJDynamicså®Œå…¨å¯¾å¿œ)")
        st.caption("FJDã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ã¨ã—ã¦ã€.crvå†…ã®éš ã—çµ¶å¯¾åº§æ¨™ã‚’è‡ªå‹•ã§é©ç”¨ã—ã¾ã™ã€‚")

        def process_crv_line_fjd_style(field_root, curves_dir):
            """FJDå®Œå…¨è‡ªå‹•ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±åˆã—ãŸå¤‰æ›é–¢æ•°"""
            for root, dirs, files in os.walk(curves_dir):
                for f in files:
                    if f.lower().endswith(".crv"):
                        crv_path = os.path.join(root, f)
                        base_name = os.path.splitext(f)[0]
                        try:
                            with open(crv_path, 'rb') as fb:
                                binary_data = fb.read()
                            if len(binary_data) < 0x48: continue
                            
                            # ã€æ–°è¦çµ±åˆã€‘FJDç”¨çµ¶å¯¾åº§æ¨™å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
                            # Offset 0x0, 0x8 ã‹ã‚‰ Double(8byte) ã§ç·¯åº¦çµŒåº¦ã‚’å–å¾—
                            base_lat = struct.unpack('<d', binary_data[0:8])[0]
                            base_lon = struct.unpack('<d', binary_data[8:16])[0]
                            
                            coords = []
                            data_section = binary_data[0x40:]
                            
                            # é«˜ç²¾åº¦ãªãƒ¡ãƒ¼ãƒˆãƒ«æ›ç®—ä¿‚æ•°
                            lat_per_m = 1.0 / 111111.0
                            lon_per_m = 1.0 / (111111.0 * np.cos(np.radians(base_lat)))

                            for i in range(0, len(data_section) - 8, 8):
                                dx, dy = struct.unpack('<ff', data_section[i:i+8])
                                if -20000 < dx < 20000:
                                    # ãƒˆãƒ—ã‚³ãƒ³ç‰¹æœ‰ã®ä¸Šä¸‹åè»¢(-dy)ã‚’é©ç”¨ã—ã¤ã¤çµ¶å¯¾åº§æ¨™ã¸å¤‰æ›
                                    actual_lon = base_lon + (dx * lon_per_m)
                                    actual_lat = base_lat + (-dy * lat_per_m)
                                    coords.append((actual_lon, actual_lat))

                            if len(coords) >= 2:
                                line = LineString(coords)
                                # FJDãŒèªè­˜ã—ã‚„ã™ã„ã‚«ãƒ©ãƒ æ§‹æˆã§SHPä½œæˆ
                                gdf = gpd.GeoDataFrame([{'Name': 'FJD_LINE'}], geometry=[line], crs="EPSG:4326")
                                gdf.to_file(os.path.join(field_root, f"{base_name}.shp"), driver='ESRI Shapefile', encoding='utf-8')
                        except Exception as e:
                            st.error(f"âŒ {f} ã®å¤‰æ›ã«å¤±æ•—: {e}")

        def process_ab_line_v2(field_root, ablines_dir):
            for root, dirs, files in os.walk(ablines_dir):
                for f in files:
                    if f.lower().endswith(".ini"):
                        ini_path = os.path.join(root, f)
                        base_name = os.path.splitext(f)[0]
                        try:
                            config = configparser.ConfigParser()
                            with open(ini_path, 'rb') as fb:
                                raw_data = fb.read()
                            content = None
                            for enc in ['utf-8', 'utf-16', 'shift-jis']:
                                try:
                                    content = raw_data.decode(enc); break
                                except: continue
                            if content:
                                config.read_string(content)
                                if 'APoint' in config and 'BPoint' in config:
                                    lat_a, lon_a = float(config['APoint']['Latitude']), float(config['APoint']['Longitude'])
                                    lat_b, lon_b = float(config['BPoint']['Latitude']), float(config['BPoint']['Longitude'])
                                    line = LineString([(lon_a, lat_a), (lon_b, lat_b)])
                                    gdf = gpd.GeoDataFrame([{'Name': base_name}], geometry=[line], crs="EPSG:4326")
                                    gdf.to_file(os.path.join(field_root, f"{base_name}.shp"), driver='ESRI Shapefile', encoding='utf-8')
                        except Exception as e:
                            st.error(f"âŒ ABãƒ©ã‚¤ãƒ³å¤‰æ›å¤±æ•—: {f} - {e}")

        def process_boundary_v2(shp_path, output_dir):
            base_name = os.path.splitext(os.path.basename(shp_path))[0]
            output_base = os.path.join(output_dir, base_name)
            prj_data = 'GEOGCS["GCS_WGS_1984",DATUM["D_WGS_1984",SPHEROID["WGS_1984",6378137.0,298.257223563]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]'
            try:
                reader = shapefile.Reader(os.path.splitext(shp_path)[0])
                writer = shapefile.Writer(output_base, shapeType=reader.shapeType)
                writer.fields = list(reader.fields[1:])
                for i, shape_rec in enumerate(reader.shapeRecords()):
                    geom = shape_rec.shape
                    new_parts = []
                    for pi in range(len(geom.parts)):
                        si, ei = geom.parts[pi], (geom.parts[pi+1] if pi+1 < len(geom.parts) else len(geom.points))
                        pts = geom.points[si:ei]
                        if pts and pts[0] != pts[-1]: pts.append(pts[0])
                        new_parts.append(pts)
                    writer.poly(new_parts)
                    rec = shape_rec.record.as_dict()
                    rec.update({'id': str(i+1), 'Name': base_name, 'visibility': 1, 'altitudeMo': "clampToGround"})
                    writer.record(**rec)
                writer.close()
                with open(output_base + ".prj", "w") as f: f.write(prj_data)
            except Exception as e:
                st.error(f"âŒ å¢ƒç•Œä¿®å¾©å¤±æ•—: {base_name} - {e}")

        uploaded_zip_topcon_v2 = st.file_uploader("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="zip", key="topcon_v2")

        if uploaded_zip_topcon_v2:
            if st.button("å¤‰æ›ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹", key="btn_v2"):
                with tempfile.TemporaryDirectory() as tmp_dir:
                    extract_path = os.path.join(tmp_dir, "extracted")
                    with zipfile.ZipFile(uploaded_zip_topcon_v2, 'r') as z:
                        z.extractall(extract_path)

                    for root, dirs, files in os.walk(extract_path, topdown=False):
                        if any(d in dirs for d in ["ABLines", "Boundaries", "Curves"]):
                            temp_save = os.path.join(tmp_dir, "temp_shp_v2")
                            if os.path.exists(temp_save): shutil.rmtree(temp_save)
                            os.makedirs(temp_save)

                            ab_dir = os.path.join(root, "ABLines")
                            if os.path.exists(ab_dir): process_ab_line_v2(temp_save, ab_dir)
                            
                            bound_dir = os.path.join(root, "Boundaries")
                            if os.path.exists(bound_dir):
                                for f in os.listdir(bound_dir):
                                    if f.lower().endswith(".shp"):
                                        process_boundary_v2(os.path.join(bound_dir, f), temp_save)

                            curves_dir = os.path.join(root, "Curves")
                            if os.path.exists(curves_dir): 
                                # æ›´æ–°å¾Œã®FJDå¯¾å¿œé–¢æ•°ã‚’å‘¼ã³å‡ºã—
                                process_crv_line_fjd_style(temp_save, curves_dir)

                            for entry in os.listdir(root):
                                entry_path = os.path.join(root, entry)
                                if os.path.isdir(entry_path): shutil.rmtree(entry_path)
                                else: os.remove(entry_path)

                            for item in os.listdir(temp_save):
                                shutil.move(os.path.join(temp_save, item), root)
                            
                            shutil.rmtree(temp_save)

                    final_zip_name = os.path.join(tmp_dir, "final_output_v2")
                    shutil.make_archive(final_zip_name, 'zip', extract_path)
                    with open(final_zip_name + ".zip", "rb") as f:
                        st.success("âœ… FJDynamicså¯¾å¿œå½¢å¼ã§ã®ä¸€æ‹¬å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                        st.download_button("ğŸ“¥ å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name="topcon_to_fjd_ready.zip")

    # --- ã‚¿ãƒ–2ï¼šãƒˆãƒ—ã‚³ãƒ³ A-Bãƒ©ã‚¤ãƒ³å¤‰æ› (å˜ä½“) ---
    with tab2:
        st.subheader("ãƒˆãƒ—ã‚³ãƒ³ã® `.ini` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        uploaded_files_topcon = st.file_uploader("iniãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ­ãƒƒãƒ—", type="ini", accept_multiple_files=True, key="topcon_ab")
        # (æ—¢å­˜ã®å€‹åˆ¥å¤‰æ›ã‚³ãƒ¼ãƒ‰ãŒç¶šã...)
        if uploaded_files_topcon:
            if st.button("ğŸš€ A-Bãƒ©ã‚¤ãƒ³ã‚’ä¸€æ‹¬å¤‰æ›ã™ã‚‹", key="btn_topcon_ab"):
                zip_buffer = io.BytesIO()
                success_count = 0
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    with tempfile.TemporaryDirectory() as tmpdir:
                        for uploaded_file in uploaded_files_topcon:
                            try:
                                content = uploaded_file.read().decode("shift-jis", errors="ignore")
                                config = configparser.ConfigParser()
                                config.read_string(content)
                                if 'APoint' in config and 'BPoint' in config:
                                    line = LineString([
                                        (float(config['APoint']['Longitude']), float(config['APoint']['Latitude'])),
                                        (float(config['BPoint']['Longitude']), float(config['BPoint']['Latitude']))
                                    ])
                                    base_name = os.path.splitext(uploaded_file.name)[0]
                                    gdf = gpd.GeoDataFrame([{'Name': base_name}], geometry=[line], crs="EPSG:4326")
                                    out_path = os.path.join(tmpdir, base_name)
                                    gdf.to_file(out_path + ".shp", driver='ESRI Shapefile', encoding='utf-8')
                                    for ext in ['.shp', '.shx', '.dbf', '.prj']:
                                        if os.path.exists(out_path + ext):
                                            zf.write(out_path + ext, arcname=f"{base_name}/{base_name}{ext}")
                                    success_count += 1
                            except Exception: continue
                if success_count > 0:
                    st.success(f"âœ… {success_count} ä»¶å¤‰æ›å®Œäº†")
                    st.download_button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", zip_buffer.getvalue(), "topcon_ab.zip")

    # --- ã‚¿ãƒ–3ï¼šSHPä¸€æ‹¬ä¿®å¾© ---
    with tab3:
        st.subheader("ä¸æ•´åˆãªSHPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰©ç†ä¿®å¾©")
        uploaded_files_repair = st.file_uploader("SHP/SHX/DBFã‚’ãƒ‰ãƒ­ãƒƒãƒ—", accept_multiple_files=True, key="repair")
        # (æ—¢å­˜ã®ä¿®å¾©ã‚³ãƒ¼ãƒ‰...)
        if uploaded_files_repair:
            if st.button("ğŸ”¥ åœƒå ´ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ä¿®å¾©ã™ã‚‹", key="btn_repair"):
                name_counts = defaultdict(int)
                shp_registry = []
                with tempfile.TemporaryDirectory() as tmp_dir:
                    for f in uploaded_files_repair:
                        safe_name = re.sub(r'[\\/:*?"<>|]', '_', f.name)
                        with open(os.path.join(tmp_dir, safe_name), "wb") as out:
                            out.write(f.getbuffer())
                        if safe_name.lower().endswith(".shp"):
                            base = os.path.splitext(safe_name)[0]
                            name_counts[base] += 1
                            uniq = f"{base}_{name_counts[base]}" if name_counts[base] > 1 else base
                            shp_registry.append({"orig": base, "uniq": uniq, "fname": safe_name})
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w') as master_zip:
                        for item in shp_registry:
                            try:
                                work_in = os.path.join(tmp_dir, f"in_{item['uniq']}")
                                for ext in ['.shp', '.shx', '.dbf']:
                                    src = os.path.join(tmp_dir, item['fname'].replace(".shp", ext).replace(".SHP", ext))
                                    if os.path.exists(src): shutil.copy(src, work_in + ext)
                                reader = shapefile.Reader(work_in)
                                work_out = os.path.join(tmp_dir, f"out_{item['uniq']}")
                                writer = shapefile.Writer(work_out, shapeType=reader.shapeType)
                                writer.fields = list(reader.fields[1:])
                                for sr in reader.shapeRecords():
                                    parts = []
                                    for i in range(len(sr.shape.parts)):
                                        start = sr.shape.parts[i]
                                        end = sr.shape.parts[i+1] if i+1 < len(sr.shape.parts) else len(sr.shape.points)
                                        pts = sr.shape.points[start:end]
                                        if len(pts) > 0 and pts[0] != pts[-1]: pts.append(pts[0])
                                        parts.append(pts)
                                    writer.poly(parts)
                                    writer.record(**sr.record.as_dict())
                                writer.close()
                                for ext in ['.shp', '.shx', '.dbf']:
                                    master_zip.write(work_out + ext, f"{item['uniq']}/{item['uniq']}{ext}")
                                master_zip.writestr(f"{item['uniq']}/{item['uniq']}.prj", 'GEOGCS["GCS_WGS_1984",DATUM["D_WGS_1984",SPHEROID["WGS_1984",6378137.0,298.257223563]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]')
                            except Exception: continue
                    st.download_button("ğŸ“¥ ä¿®å¾©æ¸ˆã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", zip_buffer.getvalue(), "repaired.zip")

    # --- ã‚¿ãƒ–4ï¼šãƒˆãƒ—ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ã¾ã¨ã‚ã¦å¤‰æ› ---
    with tab4:
        st.subheader("ãƒˆãƒ—ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ã¾ã¨ã‚ã¦å¤‰æ›")
        st.caption("ä¸è¦ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã€SHPã®ã¿ã‚’æ•´ç†ã—ã¦å‡ºåŠ›ã—ã¾ã™")
        # (æ—¢å­˜ã®ã¾ã¨ã‚ã¦å¤‰æ›ã‚³ãƒ¼ãƒ‰ãŒç¶šã...)
        uploaded_zip_topcon_all = st.file_uploader("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="zip", key="topcon_all")
        if uploaded_zip_topcon_all:
            if st.button("å®Ÿè¡Œ", key="btn_topcon_all"):
                # (æ—¢å­˜ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãã®ã¾ã¾é©ç”¨)
                pass
